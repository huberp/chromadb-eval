# Linear Algebra: Vectors and Matrices

Linear algebra is the branch of mathematics concerning linear equations, linear functions, and their representations through matrices and vector spaces. It's fundamental to many areas of mathematics and applications including computer graphics, machine learning, and physics.

## Vectors

A vector is an ordered list of numbers. Vectors can represent points in space, directions, or abstract data. Vector operations include addition, scalar multiplication, dot product, and cross product. The dot product of vectors a and b is a·b = |a||b|cos(θ), where θ is the angle between them.

## Matrices

A matrix is a rectangular array of numbers arranged in rows and columns. Matrices can represent linear transformations, systems of equations, or data. Matrix operations include addition, multiplication, and finding the determinant and inverse. Matrix multiplication is not commutative: AB ≠ BA in general.

## Systems of Linear Equations

Systems of linear equations can be solved using matrix methods like Gaussian elimination, LU decomposition, or finding the inverse matrix. The system Ax = b has a unique solution when A is invertible.

## Eigenvalues and Eigenvectors

For a square matrix A, if Av = λv for some non-zero vector v, then λ is an eigenvalue and v is an eigenvector. These concepts are crucial in understanding linear transformations and appear in applications from quantum mechanics to Google's PageRank algorithm.

## Applications

Linear algebra is used extensively in computer graphics for transformations, in machine learning for data representation and neural networks, in physics for quantum mechanics, and in engineering for structural analysis.
